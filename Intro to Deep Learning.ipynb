{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python / Jupyter Notebook / Deep Learning Introduction\n",
    "\n",
    "University of Evansville\n",
    "--------------\n",
    "+ Keenen Cates\n",
    "+ kc235@evansville.edu\n",
    "+ ACM\n",
    "\n",
    "\n",
    "Hello! This notebook serves as an introduction to the power python programming language through the useful jupyter notebooks. Throughout this notebook we will build a library for building a simplistic neural network. The only prior knowledge required to follow along is basic algebraic knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will learn and implement the fundamental components of a neural network, then add learning to our architecture.\n",
    "\n",
    "To start we will need the basic building blocks of our network, vectors. Let us start by implementing our vector library. This will give us an understanding of the math happening, while also allowing us to practice our Python skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalar_multiply(vec_a, scalar):\n",
    "    #TODO implement scalar multiplication\n",
    "    #     (2) * (1, 2, 3) = (2, 4, 6)\n",
    "    pass # remove this\n",
    "    \n",
    "def elementwise_addition(vec_a, vec_b):\n",
    "    #TODO implement elementwise addition i.e\n",
    "    #     (1, 2, 3) .+ (4, 5, 6) = (1 + 4, 2 + 5, 3 + 6)\n",
    "    #                            = (5, 7, 9)\n",
    "    pass # remove this\n",
    "\n",
    "def elementwise_multiplication(vec_a, vec_b):\n",
    "    #TODO implement elementwise multiplication i.e\n",
    "    #     (1, 2, 3) .* (4, 5, 6) = (1 * 4, 2 * 5, 3 * 6)\n",
    "    #                            = (4, 10, 18)\n",
    "    pass # remove this\n",
    "\n",
    "def vector_sum(vec_a):\n",
    "    #TODO implement vector summation\n",
    "    #     sum(1, 2, 3) = 1 + 2 + 3 \n",
    "    #                  = 6\n",
    "    pass # remove this\n",
    "\n",
    "def vector_average(vec_a):\n",
    "    #TODO implement vector averaging\n",
    "    #     avg(1, 2, 3) = sum(1, 2, 3) / len(1, 2, 3) \n",
    "    #                  = 6 / 3 \n",
    "    #                  = 2\n",
    "    pass # remove this\n",
    "\n",
    "def dot_product(vec_a, vec_b):\n",
    "    #TODO implement vector dot products\n",
    "    #     (1, 2, 3) * (4, 5, 6) = 1 * 4 + 2 * 5 + 3 * 6\n",
    "    #                           = 4 + 10 + 18\n",
    "    #                           = 32\n",
    "    pass # remove this\n",
    "\n",
    "#### EXTRA CREDIT\n",
    "#### Higher Orderism !\n",
    "\n",
    "#map\n",
    "def elementwise_operator(vec_a, unary_func):\n",
    "    ## Apply a unary function to the vector\n",
    "    pass\n",
    "\n",
    "#fold left\n",
    "def elementwise_accumulator(vec_a acc, binary_func):\n",
    "    ## Preform binary function from left to right and accumulate into acc\n",
    "    ## Return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tests():\n",
    "    assert(scalar_multiply((1, 2, 3), 2) == (2, 4, 6))\n",
    "    assert(elementwise_addition((1, 2, 3), (4, 5, 6)) == (5, 7, 9))\n",
    "    assert(elementwise_multiplication((1, 2, 3), (4, 5, 6)) == (4, 10, 18))\n",
    "    assert(vector_sum((1, 2, 3)) == 6)\n",
    "    assert(vector_average((1, 2, 3)) == 2)\n",
    "    assert(dot_product((1, 2, 3), (4, 5, 6)) == 32)\n",
    "run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Simple Neuron\n",
    "\n",
    "Neurons to put simply, are a weighted sum of inputs  with the addition of the neurons bias. I.E. The sum of (xi * wi) + b. If a threshold is hit the neuron fires.\n",
    "\n",
    "First let us implement a simple forward pass through a neuron. Use your vector library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, weights, bias=0):\n",
    "    \"\"\"\n",
    "    multi_input_network: makes a prediction \n",
    "    based on multiple inputs.\n",
    "    \n",
    "    Takes multiple inputs and multiplies \n",
    "    them times their weights.\n",
    "    \"\"\"\n",
    "    ## TODO: IMPLEMENT THIS FUNCTIOn\n",
    "    pass ## remove this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation\n",
    "Another major element of our network is activation. Our neuron only fires when it hits a certain threshold! This can be modeled using the step function. To do this let's make a a function that only fires if a threshold is hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_activation(threshold):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Activation\n",
    "However this step function is all that useful. The type of activation function you want to use depends on the application. However in this case the sigmoid models the neuron fairly well, while also being easy to differentiate (which will be important later).\n",
    "\n",
    "Here is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "def sigmoid(x):\n",
    "    return 1/(1+exp(-x))\n",
    "\n",
    "##Your turn! What is the derivative?\n",
    "def deriv_sigmoid(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Randomized Weights\n",
    "When we are learning, we don't know what the weights are. In fact this is how we learn how to represent functions! To do this lets make a helper function that gives us random weights. We want the random numbers to come from a uniform distribution with mean 0. https://docs.python.org/3/library/random.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-0a29bfb71977>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-0a29bfb71977>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    def uniform_random_weights(size, mean=0){\u001b[0m\n\u001b[0m                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def uniform_random_weights(size):\n",
    "    \"\"\"\n",
    "    Takes a size of desired weight vector, \n",
    "    and outputs a vector of the same size\n",
    "    with uniform random weights\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning\n",
    "Learning in a network essential means that we will be tuning our weights so that we get the right answer. Let's use our tools to solve this problem.\n",
    "\n",
    "Let's say we have a matrix and we want to use the Input to guess the output. Let's build a network that learns how to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IN    | OUT\n",
      "0 0 1 | 0\n",
      "0 1 1 | 1\n",
      "1 0 1 | 1\n",
      "1 1 1 | 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "IN    | OUT\n",
    "0 0 1 | 0\n",
    "0 1 1 | 1\n",
    "1 0 1 | 1\n",
    "1 1 1 | 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[0, 0, 1],\n",
    "     [0, 1, 1],\n",
    "     [1, 0, 1],\n",
    "     [1, 1, 1]]\n",
    "\n",
    "Y = [0,\n",
    "     1,\n",
    "     1,\n",
    "     0]\n",
    "\n",
    "#Seed the PRNG so that we get deterministic results\n",
    "random.seed(0)\n",
    "\n",
    "#TODO: initialize our weights as syn0\n",
    "\n",
    "epochs = 10000\n",
    "for i in range(epochs):\n",
    "    #TODO: Set 'layer0' to the inputs\n",
    "    #TODO: Set 'layer1' to a forward pass over all the input vectors using syn0\n",
    "    #TODO: Apply activation to each input\n",
    "    #TODO: Check the error of each row, save as layer1_error\n",
    "    \n",
    "    #UPDATING OUR MODEL\n",
    "    #We can now use the slope of the sigmoid to find out how to change our weights to fix our model\n",
    "    #I.e set layer1_delta to the error times the slope\n",
    "    #Update each weight by the delta dot layer0\n",
    "    \n",
    "#print(\"Our output is...\")\n",
    "#print(l1)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
